{"cells":[{"cell_type":"code","execution_count":null,"id":"f5b65469","metadata":{"papermill":{"duration":9.013947,"end_time":"2022-03-11T08:42:48.331815","exception":false,"start_time":"2022-03-11T08:42:39.317868","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# unicode arabic \n","# https://unicode.org/charts/PDF/U0600.pdf\n","# https://unicode.org/charts/PDF/UFE70.pdf\n","\n","# mazajak cbow\n","# http://mazajak.inf.ed.ac.uk:8000/get_cbow_250\n","# http://mazajak.inf.ed.ac.uk:8000/get_sg_250\n","\n","# !python3 -m pip install farasapy\n"]},{"cell_type":"code","execution_count":null,"id":"7a662f7d","metadata":{"papermill":{"duration":5.217074,"end_time":"2022-03-11T08:42:53.578374","exception":false,"start_time":"2022-03-11T08:42:48.3613","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import json\n","import os\n","import matplotlib.pyplot as plt\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import random\n","import re\n","import requests\n","import tensorflow as tf\n","from nltk.corpus import stopwords\n","from nltk.tokenize import RegexpTokenizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split, cross_validate\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import make_pipeline\n","from typing import Callable, Tuple, Union\n","from pylab import rcParams\n","\n","rcParams[\"figure.figsize\"] = (16,9)\n","\n","try:\n","    STOP_WORDS_BASIC = stopwords.words(\"arabic\")\n","except LookupError:\n","    nltk.download(\"stopwords\")\n","    STOP_WORDS_BASIC = stopwords.words(\"arabic\")\n","\n","# from farasa.segmenter import FarasaSegmenter\n","random.seed(42)\n","np.random.seed(42)\n","\n","DATA_PATH_CSV = \"../input/dialect-dataset/\"\n","# DATA_PATH_PKL = \"../input/dialectdataset/\"\n","DATA_PATH_PKL = \"../input/aim-task/\"\n","DATA_PATH_OUT = \"../working/\"\n","AIM_URI = \"https://recruitment.aimtechnologies.co/ai-tasks\"\n","ID_LIMIT = 1000\n","STOP_WORDS_BASIC = stopwords.words(\"arabic\")\n","FILENAME = \"dialect_dataset\"\n","MAX_LENGTH = 1024  # FIXME should be determined by EDA\n"]},{"cell_type":"markdown","id":"4fef7c2d","metadata":{"papermill":{"duration":0.02819,"end_time":"2022-03-11T08:42:53.634957","exception":false,"start_time":"2022-03-11T08:42:53.606767","status":"completed"},"tags":[]},"source":["---\n","\n","## functions for fetching data\n"]},{"cell_type":"code","execution_count":null,"id":"4e1b01ab","metadata":{"papermill":{"duration":0.042801,"end_time":"2022-03-11T08:42:53.705192","exception":false,"start_time":"2022-03-11T08:42:53.662391","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def fetch_tweets_by_id(ids: list, object_hook: Callable = None) -> dict:\n","    \"\"\"preforms a POST request to the AIM_URI, and returns the dictionary of\n","        id:tweet on success, raises an Exception on failure. The function\n","        handles the limit for the AIM_URI using `ID_LIMIT` constant\n","\n","    Parameters\n","    ----------\n","    ids: array-like\n","        a list or array holding the IDs to retrieve, each is of type str\n","\n","    Returns\n","    -------\n","    out: dict\n","        a dictionary where the keys are the given IDs and the corresponding\n","        values are the documents\n","\n","    Raises\n","    ------\n","    FIXME add proper raises\n","    \"\"\"\n","    tweets = {}\n","    for i in range(0, len(ids), ID_LIMIT):\n","        data = json.dumps(ids[i : i + ID_LIMIT])\n","        res = requests.post(AIM_URI, data)\n","        if res.ok:\n","            res.encoding = \"utf8\"\n","            i_docs = json.loads(res.text, object_hook=object_hook)\n","            tweets.update(i_docs)\n","        else:\n","            raise Exception(\"FIXME: check status code of response\")\n","    return tweets\n","\n","\n","def check_cached_data() -> bool:\n","    \"\"\"checks if the data has been loaded before and cached\n","\n","    FIXME complete the pydoc\n","    \"\"\"\n","    filename = FILENAME + \".pkl\"\n","    for _, _, filenames in os.walk(DATA_PATH_PKL):\n","        if filename in filenames:\n","            return True\n","    return False\n","\n","\n","def load_local_dataset() -> pd.DataFrame:\n","    \"\"\"loads the local dataset\n","\n","    FIXME complete the pydoc\n","    \"\"\"\n","    file_path = os.path.join(DATA_PATH_CSV, FILENAME + \".csv\")\n","    df = pd.read_csv(file_path, index_col=[\"id\"])\n","    df[\"dialect\"] = df[\"dialect\"].astype(\"category\")\n","    return df\n","\n","\n","def fetch_remote_dataset(ids: list, object_hook: Callable = None) -> pd.DataFrame:\n","    \"\"\"fetches the dataset by POSTing to URI\n","\n","    FIXME complete the pydoc\n","    \"\"\"\n","    tweets = fetch_tweets_by_id(ids, object_hook=object_hook)\n","    return pd.DataFrame(\n","        tweets.values(), index=[int(k) for k in tweets.keys()], columns=[\"tweets\"]\n","    )\n","\n","\n","def load_full_dataset(\n","    force: bool = False, cache: bool = True, object_hook: Callable = None\n",") -> pd.DataFrame:\n","    \"\"\"loads the full dataset\n","\n","    # FIXME complete the pydoc\n","    \"\"\"\n","    if not force and check_cached_data():\n","        cached_file_path = os.path.join(DATA_PATH_PKL, FILENAME + \".pkl\")\n","        return pickle.load(open(cached_file_path, \"rb\"))\n","    else:\n","        local_df = load_local_dataset()\n","        ids = local_df.index.values.astype(str).tolist()\n","        remote_df = fetch_remote_dataset(ids, object_hook=object_hook)\n","        full_df = local_df.join(remote_df, on=local_df.index)\n","        if cache:\n","            cached_file_path = os.path.join(DATA_PATH_OUT, FILENAME + \".pkl\")\n","            pickle.dump(full_df, open(cached_file_path, \"wb\"))\n","        return full_df\n"]},{"cell_type":"markdown","id":"f230ad1d","metadata":{},"source":["## Function for loading saved `sklearn` models\n"]},{"cell_type":"code","execution_count":null,"id":"2509f51b","metadata":{"trusted":true},"outputs":[],"source":["def load_classic_model(name: str = \"benchmark\"):\n","    \"\"\"loads a classic ML model saved as `.pkl` pickled file\n","\n","    Parameters:\n","    -----------\n","\n","    Returns:\n","    --------\n","\n","    Raises:\n","    -------\n","\n","    TODO complete the pydoc\n","    \"\"\"\n","    path = os.path.join(DATA_PATH_PKL, f\"models/{name}.pkl\")\n","    if not os.path.isfile(path):\n","        raise FileNotFoundError(f\"Could not find model {name}!\")\n","    return pickle.load(open(path, \"rb\"))\n"]},{"cell_type":"markdown","id":"0a314984","metadata":{},"source":["## Function to save validation set to disk\n"]},{"cell_type":"code","execution_count":null,"id":"68b75138","metadata":{"papermill":{"duration":0.03469,"end_time":"2022-03-11T08:42:53.768748","exception":false,"start_time":"2022-03-11T08:42:53.734058","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def store_validation_set(\n","    df: pd.DataFrame, set_size: float = 0.2\n",") -> Tuple[pd.DataFrame, pd.DataFrame]:\n","    \"\"\"stores a copy of validation set on disk\n","\n","    FIXME complete pydoc\n","    \"\"\"\n","    y = full_df[\"dialect\"]\n","    train_df, valid_df = train_test_split(\n","        full_df, test_size=set_size, random_state=42, stratify=y\n","    )\n","    valid_set_path = os.path.join(DATA_PATH_OUT, \"valid.pkl\")\n","    pickle.dump(valid_df, open(valid_set_path, \"wb\"))\n","    return train_df, valid_df\n"]},{"cell_type":"markdown","id":"00fe3025","metadata":{"papermill":{"duration":0.026834,"end_time":"2022-03-11T08:42:53.822791","exception":false,"start_time":"2022-03-11T08:42:53.795957","status":"completed"},"tags":[]},"source":["--- \n","\n","## Fetching the data\n"]},{"cell_type":"code","execution_count":null,"id":"b6c7e6e4","metadata":{"papermill":{"duration":525.048162,"end_time":"2022-03-11T08:51:38.898775","exception":false,"start_time":"2022-03-11T08:42:53.850613","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["full_df = load_full_dataset()\n","# keep a validation set aside\n","df, _ = store_validation_set(full_df)\n","\n","del _\n"]},{"cell_type":"markdown","id":"c996a92c","metadata":{"papermill":{"duration":0.02832,"end_time":"2022-03-11T08:51:38.956967","exception":false,"start_time":"2022-03-11T08:51:38.928647","status":"completed"},"tags":[]},"source":["---\n","\n","## preprocessing\n"]},{"cell_type":"code","execution_count":null,"id":"f21451c0","metadata":{"papermill":{"duration":0.034775,"end_time":"2022-03-11T08:51:39.019178","exception":false,"start_time":"2022-03-11T08:51:38.984403","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["REGEX_HANDLER = r\"(?u)@\\w+\"\n","REGEX_HYPERLINK = r\"(?u)https?://\\w+\"\n","REGEX_HASHTAG = r\"(?u)#(\\w+)\"\n","# to capture letters that repeat over 3 times\n","REGEX_CHAR_3_PLUS = r\"(?u)(?=(\\w))\\1{3,}\"\n","\n","# to capture non-arabic, & non-farsi unicodes\n","REGEX_NOT_ARA_IRA = r\"(?u)\\b[^\\u0600-\\u06ff\\ufe70-\\ufefc_]+\\b\"\n","\n","# to capture most common arabic glyphs\n","REGEX_RANGE_ARA = r\"\\ufe70-\\ufefc\\u0621-\\u063a\\u0640-\\u0652\\u060c\\u061f\"\n","REGEX_NOT_ARA = r\"(?u)\\b[^\"+ REGEX_RANGE_ARA + \"]+\\b\"\n","\n","# converting farsi glyphs into arabic\n","REGEX_IRA2ARA = {\n","    r\"\\u06fd\": \"\\u0621\",\n","    r\"[\\u0676\\u0677]\": \"\\u0624\",\n","    r\"[\\u0678\\u06d3]\": \"\\u0626\",\n","    r\"[\\u0622\\u0623\\u0625\\u0671-\\u0675]\": \"\\u0627\",\n","    r\"[\\u067b\\u067e\\u0680]\": \"\\u0628\",\n","    r\"[\\u067a\\u067c\\u067d\\u067f]\": \"\\u062a\",\n","    r\"[\\u0686\\u0687]\": \"\\u062c\",\n","    r\"[\\u0688-\\u068b\\u068d]\": \"\\u062f\",\n","    r\"[\\u068c\\u068e-\\u0690]\": \"\\u0630\",\n","    r\"[\\u066b\\u0691-\\u0695]\": \"\\u0631\",\n","    r\"[\\u0696-\\u0699]\": \"\\u0632\",\n","    r\"[\\u069a\\u069b]\": \"\\u0633\",\n","    r\"\\u069c\": \"\\u0634\",\n","    r\"\\u060f\": \"\\u0639\",\n","    r\"\\u06d4\": \"\\u0640\",\n","    r\"[\\u06a4\\u06a5]\": \"\\u0641\",\n","    r\"\\u06a8\\u06a6\": \"\\u0642\",\n","    r\"[\\u063b\\u063c\\u06a9-\\u06b4]\": \"\\u0643\",\n","    r\"[\\u06b5-\\u06b8]\": \"\\u0644\",\n","    r\"\\u06fe\": \"\\u0645\",\n","    r\"[\\u06b9-\\u06bd]\": \"\\u0646\",\n","    r\"[\\u0629\\u06be\\u06c0-\\u06c3\\u06ff]\": \"\\u0647\",\n","    r\"[\\u06c4-\\u06cb\\u06cf]\": \"\\u0648\",\n","    r\"[\\u0620\\u063d-\\u063f\\u0649\\u06cc-\\u06ce\\u06d0-\\u06d2\\u06e6\\u06e7]\": \"\\u064a\",\n","    r\"[\\u06d9\\ufef5\\ufef7\\ufef9]\": \"\\ufefb\",\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"f9171583","metadata":{"trusted":true},"outputs":[],"source":["def regex_substitute(\n","    series: Union[list, np.ndarray, pd.Series], regex_holder_dict: dict\n",") -> pd.Series:\n","    \"\"\"given an array-like, & a dictionary of RegExp:`placeholder`, the series\n","    is processed to replace each RegExp with the corresponding `placeholder`\n","\n","    Parameters:\n","    -----------\n","\n","    Returns:\n","    --------\n","\n","    Raises:\n","    -------\n","\n","    TODO complete the pydoc\n","    \"\"\"\n","    if not isinstance(series, pd.Series):\n","        try:\n","            series = pd.Series(series, dtype=str)\n","        except:\n","            raise ValueError(\"Invalid series: not array-like\")\n","    for regex, holder in regex_holder_dict.items():\n","        series = series.apply(lambda s: re.sub(regex, holder, s))\n","    return series\n","\n","\n","def preprocess(data: Union[list, np.ndarray, pd.Series, pd.DataFrame]) -> pd.Series:\n","    \"\"\"The actual preprocessing subroutine\n","\n","    TODO complete pydoc\n","\n","    \"\"\"\n","    if isinstance(data, pd.DataFrame):\n","        if \"tweets\" not in data.columns:\n","            raise Exception(\"FIXME write a proper exception\")\n","        # extract data as series\n","        data = data[\"tweets\"]\n","    elif isinstance(data, list) or isinstance(data, np.ndarray):\n","        data = pd.Series(data, dtype=str)\n","    elif not isinstance(data, pd.Series):\n","        raise Exception(\"FIXME write a proper exception\")\n","\n","    # removing twitter specific features\n","    regex_twitter = {\n","        REGEX_HANDLER: r\" \",\n","        REGEX_HYPERLINK: r\" \",\n","        r\"\\d+\": r\" \",\n","        REGEX_HASHTAG: r\"\\1\",\n","        r\"_\": r\" \",\n","    }\n","    data = regex_substitute(data, regex_twitter)\n","\n","    #     normalising the characters\n","    data = regex_substitute(data, REGEX_IRA2ARA)\n","\n","    # removing any non-useful tokens\n","    extra_tokens = {\n","        REGEX_NOT_ARA: r\" \",\n","        r\"(?u)[\\d\\s_]+\": r\" \",\n","        REGEX_CHAR_3_PLUS: r\"\\1\\1\",\n","    }\n","    ara_ira = regex_substitute(data, extra_tokens)\n","    data = ara_ira.str.strip()\n","\n","    # FIXME could use lemmatisation?\n","\n","    return data\n","\n","\n","def init_preprocess(\n","    test_size: float = 0.1,\n",") -> Tuple[pd.Series, pd.Series, pd.Series, pd.Series]:\n","    \"\"\"initialisation point for the training of the model\n","\n","    TODO complete the pydoc\n","    \"\"\"\n","    full_df = load_full_dataset()\n","    # keep a validation set aside\n","    df, _ = store_validation_set(full_df)\n","\n","    df[\"tweets\"] = preprocess(df[\"tweets\"])\n","\n","    # split data\n","    X = df[\"tweets\"]\n","    y = df[\"dialect\"]\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=test_size, random_state=42, stratify=y\n","    )\n","    del df\n","\n","    # TODO do more preprocess\n","\n","    return X_train, X_test, y_train, y_test\n","\n","\n","def build_tokeniser(\n","    corpus: Union[list, np.ndarray, pd.Series]\n",") -> tf.keras.preprocessing.text.Tokenizer:\n","    \"\"\"instantiate and fits a `tensorflow.keras.preprocessing.Tokenizer` object\n","    on the given corpus, and returns the tokeniser\n","\n","    Parameters:\n","    -----------\n","\n","    TODO complete pydoc\n","    \"\"\"\n","    tokeniser = tf.keras.preprocessing.text.Tokenizer()\n","    _ = tokeniser.fit_on_texts(corpus)\n","    return tokeniser\n","\n","\n","def tokenise_pad_texts(\n","    corpus: Union[list, np.ndarray, pd.Series],\n","    tokeniser: tf.keras.preprocessing.text.Tokenizer = None,\n","    maxlen: int = MAX_LENGTH,\n","    padding: str = \"post\",\n","):\n","    \"\"\"tokenises the documents in `corpus`, then pads the documents to the\n","    `maxlen` length\n","\n","    Parameters:\n","    -----------\n","\n","    TODO complete pydoc\n","    \"\"\"\n","    if not tokeniser:\n","        tokeniser = build_tokeniser(corpus)\n","    elif not isinstance(tokeniser, tf.keras.preprocessing.text.Tokenizer):\n","        raise ValueError(\n","            \"The tokeniser must be of type tensorflow.keras.preprocessing.Tokenizer\"\n","        )\n","    tokens = tokeniser.texts_to_sequences(corpus)\n","    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences(\n","        tokens, maxlen=maxlen, padding=padding\n","    )\n","    return padded_tokens\n","\n","\n","def onehot_encode_labels(labels: Union[list, np.ndarray, pd.Series]) -> np.ndarray:\n","    \"\"\"performs one hot encoding on labels\n","\n","    Parameters:\n","    -----------\n","\n","    TODO complete pydoc\n","    \"\"\"\n","    labels = pd.Series(labels, dtype=\"category\")\n","    return tf.keras.utils.to_categorical(labels.cat.codes)\n"]},{"cell_type":"code","execution_count":null,"id":"fdc99c38","metadata":{"papermill":{"duration":0.035174,"end_time":"2022-03-11T08:51:57.190235","exception":false,"start_time":"2022-03-11T08:51:57.155061","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# segmenter = FarasaSegmenter()\n","# v_segment = np.vectorize(segmenter.segment)\n","# step = 1024\n","# tweets = df['tweets'].values\n","# lem_text = df['tweets'].apply(segmenter.segment)\n","# lem_text.sample(1)\n","# for idx in np.arange(0, df.shape[0], step):\n","#     tweets[idx:idx+step] = v_segment(tweets[idx:idx+step])\n","#     print(idx)\n"]},{"cell_type":"code","execution_count":null,"id":"2b1667dc","metadata":{"trusted":true},"outputs":[],"source":["df['dialect'].value_counts().plot.barh()\n","plt.plot()\n"]},{"cell_type":"code","execution_count":null,"id":"7fea61ce","metadata":{"trusted":true},"outputs":[],"source":["df_cpy = df.copy()\n","regex_twitter = {\n","    REGEX_HANDLER: r\"<HNDL/>\",\n","    REGEX_HYPERLINK: r\"<URL/>\",\n","    r\"\\d+\": r\"<NUM/>\",\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"3a371a45","metadata":{"trusted":true},"outputs":[],"source":["df_cpy[\"tweets\"] = regex_substitute(df_cpy[\"tweets\"], regex_twitter)\n"]},{"cell_type":"code","execution_count":null,"id":"53f4d14b","metadata":{"trusted":true},"outputs":[],"source":["df_cpy[\"len\"] = df[\"tweets\"].apply(len)\n","df_cpy[\"word_count\"] = (\n","    df[\"tweets\"].apply(lambda s: re.findall(r\"(?u)\\b\\w\\w+\\b\", s)).apply(len)\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"3eafa6da","metadata":{"trusted":true},"outputs":[],"source":["df_cpy[[\"len\", \"dialect\"]].boxplot(by=\"dialect\", grid=False)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"2c8f6fea","metadata":{"trusted":true},"outputs":[],"source":["df_cpy[[\"word_count\", \"dialect\"]].boxplot(by=\"dialect\", grid=False)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"e9fb4ac7","metadata":{"trusted":true},"outputs":[],"source":["# NOTE: the notebook might allocate too much memory\n","del df_cpy\n"]},{"cell_type":"code","execution_count":null,"id":"aec22d93","metadata":{"trusted":true},"outputs":[],"source":["X_train, X_test, y_train, y_test = init_preprocess()\n"]},{"cell_type":"markdown","id":"cc727d61","metadata":{"papermill":{"duration":0.027134,"end_time":"2022-03-11T08:51:57.868326","exception":false,"start_time":"2022-03-11T08:51:57.841192","status":"completed"},"tags":[]},"source":["--- \n","\n","## Model training\n"]},{"cell_type":"code","execution_count":null,"id":"8b0956c9","metadata":{"papermill":{"duration":0.037037,"end_time":"2022-03-11T08:52:16.800287","exception":false,"start_time":"2022-03-11T08:52:16.76325","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from itertools import chain\n","\n","\n","def tokeniser(string: str) -> list:\n","    \"\"\"https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-a-list-of-lists\n","    https://stackoverflow.com/questions/5466451/how-can-i-print-literal-curly-brace-characters-in-a-string-and-also-use-format\n","    \n","    Parameters:\n","    -----------\n","\n","    Returns:\n","    --------\n","\n","    TODO complete pydoc\n","    \"\"\"\n","    regex_c37 = r\"(?u)(?=(\\w{3}))\" + \"\".join(\n","        [f\"(?=(\\\\w{{,{i}}}))\" for i in range(4, 8)]\n","    )\n","    regex_w26 = r\"(?u)(?=(?!\\W)(?=((?:\\W*\\b\\w+\\b){2}))\" + \"\".join(\n","        [f\"(?!\\\\W)(?=((?:\\\\W*\\\\b\\w+\\\\b){{,{i}}}))\" for i in range(3, 7)]\n","    )\n","    token_c37 = RegexpTokenizer(regex_c37).tokenize(string)\n","    token_w26 = RegexpTokenizer(regex_w26).tokenize(string)\n","    tokens = set(chain(*(token_c37 + token_w26)))\n","    return sorted(tokens)\n","\n","\n","def build_naive_bayes_model(*args, **kwargs):\n","    \"\"\"builds and returns an sklearn pipeline\n","\n","\n","    TODO complete the pydoc\n","    \"\"\"\n","    if \"stop_words\" not in kwargs:\n","        kwargs[\"stop_words\"] = STOP_WORDS_BASIC\n","    model = MultinomialNB(alpha=kwargs.get(\"alpha\", 1))\n","    if \"alpha\" in kwargs:\n","        del kwargs[\"alpha\"]\n","    vectoriser = CountVectorizer(**kwargs)\n","\n","    return make_pipeline(vectoriser, model)\n"]},{"cell_type":"code","execution_count":null,"id":"604aac70","metadata":{"papermill":{"duration":18.838259,"end_time":"2022-03-11T08:52:16.734109","exception":false,"start_time":"2022-03-11T08:51:57.89585","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# building a benchmark model\n","bench = build_naive_bayes_model(stop_words=None)\n","\n","_ = bench.fit(X_train, y_train)\n","y_pred = bench.predict(X_test)\n","clf_report = classification_report(y_test, y_pred, labels=y_train.unique())\n","print(clf_report)\n","\n","# TODO train better models\n"]},{"cell_type":"code","execution_count":null,"id":"11f5a2ee","metadata":{"trusted":true},"outputs":[],"source":["# NOTE notebook might allocate too much memory\n","pickle.dump(bench, open(\"benchmark.pkl\", \"wb\"))\n","del bench\n"]},{"cell_type":"code","execution_count":null,"id":"2f6444dd","metadata":{"papermill":{"duration":0.037132,"end_time":"2022-03-11T08:52:16.866577","exception":false,"start_time":"2022-03-11T08:52:16.829445","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def analyser(*args):\n","    \"\"\"to allow BoW to be built using a combination of character &\n","    word n-grams. using `lambda` expression makes the model non-pickle friendly\n","\n","    TODO complete pydoc\n","    \"\"\"\n","    return args\n","\n","\n","combined = build_naive_bayes_model(\n","    alpha=0.1, stop_words=STOP_WORDS_BASIC, analyzer=analyser, tokenizer=tokeniser\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"79c33d6d","metadata":{"trusted":true},"outputs":[],"source":["# NOTE notebook might allocate too much memory\n","pickle.dump(combined, open(\"combined.pkl\", \"wb\"))\n","del combined\n"]},{"cell_type":"code","execution_count":null,"id":"fa055063","metadata":{"papermill":{"duration":0.037361,"end_time":"2022-03-11T08:52:16.995579","exception":false,"start_time":"2022-03-11T08:52:16.958218","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["model_w26 = build_naive_bayes_model(stop_words=STOP_WORDS_BASIC, ngram_range=(2, 6))\n"]},{"cell_type":"code","execution_count":null,"id":"0f2b8530","metadata":{"trusted":true},"outputs":[],"source":["# NOTE notebook might allocate too much memory\n","pickle.dump(model_w26, open(\"model_w26.pkl\", \"wb\"))\n","del model_w26\n"]},{"cell_type":"code","execution_count":null,"id":"60a0e7da","metadata":{"papermill":{"duration":0.035104,"end_time":"2022-03-11T08:52:16.929801","exception":false,"start_time":"2022-03-11T08:52:16.894697","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["model_c37 = build_naive_bayes_model(\n","    alpha=0.1, stop_words=STOP_WORDS_BASIC, analyzer=\"char\", ngram_range=(3, 7)\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"b41e3b17","metadata":{"trusted":true},"outputs":[],"source":["# NOTE notebook might allocate too much memory\n","pickle.dump(model_c37, open(\"model_c37.pkl\", \"wb\"))\n","del model_c37\n"]},{"cell_type":"markdown","id":"19c4ad85","metadata":{"papermill":{"duration":0.029906,"end_time":"2022-03-11T08:52:17.054104","exception":false,"start_time":"2022-03-11T08:52:17.024198","status":"completed"},"tags":[]},"source":["---\n"]},{"cell_type":"code","execution_count":null,"id":"06f5c7ec","metadata":{"papermill":{"duration":0.034486,"end_time":"2022-03-11T08:52:17.117951","exception":false,"start_time":"2022-03-11T08:52:17.083465","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["scoring = [\n","    \"neg_log_loss\",\n","    \"f1_micro\",\n","]  # , \"f1_macro\", \"precision_micro\", \"recall_micro\"]\n","X = pd.concat([X_train, X_test])\n","y = pd.concat([y_train, y_test])\n","labels = y.unique()\n"]},{"cell_type":"code","execution_count":null,"id":"a70bd98b","metadata":{"trusted":true},"outputs":[],"source":["models = [\"combined\", \"model_c37\", \"model_w26\"]\n","result = {}\n","for model in models:\n","    mod = pickle.load(open(f\"{model}.pkl\", \"rb\"))\n","    result[model] = cross_validate(mod, X, y, scoring=scoring)\n","    del mod\n","\n"]},{"cell_type":"code","execution_count":null,"id":"e7a526e0","metadata":{"trusted":true},"outputs":[],"source":["for model in result:\n","    for key, val in result[model].items():\n","        result[model][key] = np.mean(val)\n","    print(result[model])\n","\n","f1_scores = [r['test_f1_micro'] for r in result.values()]\n","neg_log_loss = [r['test_neg_log_loss'] for r in result.values()]\n","print(models[np.argmax(f1_scores)], models[np.argmin(neg_log_loss)])\n"]},{"cell_type":"code","execution_count":null,"id":"97648435","metadata":{"trusted":true},"outputs":[],"source":["model = models[np.argmax(f1_scores)]\n"]},{"cell_type":"code","execution_count":null,"id":"813e8bcd","metadata":{"trusted":true},"outputs":[],"source":["mod = pickle.load(open(f\"{model}.pkl\", \"rb\"))\n","_ = mod.fit(X_train, y_train)\n","y_pred = mod.predict(X_test)\n","clf_report = classification_report(y_test, y_pred, labels=labels)\n","print(clf_report)\n"]},{"cell_type":"code","execution_count":null,"id":"e2eea486","metadata":{"trusted":true},"outputs":[],"source":["pickle.dump(mod, open(f\"{model}.pkl\", \"wb\"))\n","\n","del mod"]},{"cell_type":"code","execution_count":null,"id":"2c843839","metadata":{"papermill":{"duration":0.037806,"end_time":"2022-03-11T09:20:26.853088","exception":false,"start_time":"2022-03-11T09:20:26.815282","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# TODO TfidfVectorizer\n","# TODO SVC\n"]},{"cell_type":"markdown","id":"5c0eedd6","metadata":{"papermill":{"duration":0.031257,"end_time":"2022-03-11T09:20:26.915972","exception":false,"start_time":"2022-03-11T09:20:26.884715","status":"completed"},"tags":[]},"source":["---\n"]},{"cell_type":"code","execution_count":null,"id":"f30c3599","metadata":{"papermill":{"duration":20.899426,"end_time":"2022-03-11T09:20:47.948168","exception":false,"start_time":"2022-03-11T09:20:27.048742","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["tknsr = build_tokeniser(pd.concat([X_train, X_test]).values)\n","X_train_padded = tokenise_pad_texts(X_train, tknsr)\n","X_test_padded = tokenise_pad_texts(X_test, tknsr)\n","\n","y1h = onehot_encode_labels(pd.concat([y_train, y_test]))\n","y_train1h = y1h[:y_train.shape[0]]\n","y_test1h = y1h[y_train.shape[0]:]\n"]},{"cell_type":"code","execution_count":null,"id":"2fb8289d","metadata":{"papermill":{"duration":2.963558,"end_time":"2022-03-11T09:20:50.943412","exception":false,"start_time":"2022-03-11T09:20:47.979854","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["model = tf.keras.models.Sequential(\n","    layers=[\n","        tf.keras.layers.Embedding(\n","            len(tknsr.word_counts) + 1,\n","            128,\n","            embeddings_initializer=tf.keras.initializers.RandomUniform(),\n","            input_length=MAX_LENGTH,\n","        ),\n","        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.25)),\n","        tf.keras.layers.Dropout(0.5),\n","        tf.keras.layers.Dense(64, activation=\"relu\"),\n","        tf.keras.layers.Dense(y_train1h.shape[1], activation=\"softmax\"),\n","    ],\n","    name=\"deep_classifier\",\n",")\n","\n","model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])\n"]},{"cell_type":"code","execution_count":null,"id":"4b47a0d9","metadata":{"papermill":{"duration":1873.037465,"end_time":"2022-03-11T09:52:04.013384","exception":false,"start_time":"2022-03-11T09:20:50.975919","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["_ = model.fit(\n","    X_train_padded,\n","    y_train1h,\n","    batch_size=32,\n","    epochs=2,\n","    verbose=1,\n","    validation_data=(X_test_padded, y_test1h),\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"f6b0865e","metadata":{"papermill":{"duration":6.732669,"end_time":"2022-03-11T09:52:17.395902","exception":false,"start_time":"2022-03-11T09:52:10.663233","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["model_json = model.to_json()\n","json.dump(json.loads(model_json), open('rnn_model.json', \"w\"))\n","model.save_weights('rnn_model_weights.h5')\n"]},{"cell_type":"code","execution_count":null,"id":"c370f689","metadata":{"papermill":{"duration":6.909444,"end_time":"2022-03-11T09:52:31.243725","exception":false,"start_time":"2022-03-11T09:52:24.334281","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# TODO transfer learning & transformers\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
